{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Material\n",
    "\n",
    "- Credits to https://realpython.com/sentiment-analysis-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "- https://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Beginning training\n",
      "Loss\tPrecision\tRecall\tF-score\n",
      "Training iteration 0\n",
      "0.2272550519555807\t0.0\t0.0\t0\n",
      "Training iteration 1\n",
      "0.13405397534370422\t0.0\t0.0\t0\n",
      "Training iteration 2\n",
      "0.08671092055737972\t0.0\t0.0\t0\n",
      "Training iteration 3\n",
      "0.06626979261636734\t0.0\t0.0\t0\n",
      "Training iteration 4\n",
      "0.017377817421220243\t0.0\t0.0\t0\n",
      "Training iteration 5\n",
      "0.030269148584920913\t0.0\t0.0\t0\n",
      "Training iteration 6\n",
      "0.008006562886293977\t0.0\t0.0\t0\n",
      "Training iteration 7\n",
      "0.0007691908704146044\t0.0\t0.0\t0\n",
      "Training iteration 8\n",
      "0.007142077175899431\t0.0\t0.0\t0\n",
      "Training iteration 9\n",
      "0.0018058250742569726\t0.0\t0.0\t0\n",
      "Training iteration 10\n",
      "0.0021237552686841354\t0.0\t0.0\t0\n",
      "Training iteration 11\n",
      "0.0026715374538071046\t0.0\t0.0\t0\n",
      "Training iteration 12\n",
      "5.032576231656094e-05\t0.0\t0.0\t0\n",
      "Training iteration 13\n",
      "4.269669997825076e-05\t0.0\t0.0\t0\n",
      "Training iteration 14\n",
      "0.00015438124046340818\t0.0\t0.0\t0\n",
      "Training iteration 15\n",
      "0.00032164324272798694\t0.0\t0.0\t0\n",
      "Training iteration 16\n",
      "2.689471824623979e-05\t0.0\t0.0\t0\n",
      "Training iteration 17\n",
      "5.809010440316342e-05\t0.0\t0.0\t0\n",
      "Training iteration 18\n",
      "5.816596363672488e-06\t0.0\t0.0\t0\n",
      "Training iteration 19\n",
      "1.8379023629222502e-05\t0.0\t0.0\t0\n",
      "Testing model\n",
      "Review text: \n",
      "Transcendently beautiful in moments outside the office, it seems almost\n",
      "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
      "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
      "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
      "doesn't matter. (The worst is sort of tedious - like Office Space with less\n",
      "humor.)\n",
      "\n",
      "Predicted sentiment: Negative\tScore: 0.9995772242546082\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "TEST_REVIEW = \"\"\"\n",
    "Transcendently beautiful in moments outside the office, it seems almost\n",
    "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
    "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
    "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
    "doesn't matter. (The worst is sort of tedious - like Office Space with less\n",
    "humor.)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "eval_list = []\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    training_data: list, test_data: list, iterations: int = 20\n",
    ") -> None:\n",
    "    # Build pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    textcat.add_label(\"pos\")\n",
    "    textcat.add_label(\"neg\")\n",
    "\n",
    "    # Train only textcat\n",
    "    training_excluded_pipes = [\n",
    "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
    "    ]\n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        # Training loop\n",
    "        print(\"Beginning training\")\n",
    "        print(\"Loss\\tPrecision\\tRecall\\tF-score\")\n",
    "        batch_sizes = compounding(\n",
    "            4.0, 32.0, 1.001\n",
    "        )  # A generator that yields infinite series of input numbers\n",
    "        for i in range(iterations):\n",
    "            print(f\"Training iteration {i}\")\n",
    "            loss = {}\n",
    "            random.shuffle(training_data)\n",
    "            batches = minibatch(training_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                text, labels = zip(*batch)\n",
    "                nlp.update(text, labels, drop=0.2, sgd=optimizer, losses=loss)\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                evaluation_results = evaluate_model(\n",
    "                    tokenizer=nlp.tokenizer,\n",
    "                    textcat=textcat,\n",
    "                    test_data=test_data,\n",
    "                )\n",
    "                print(\n",
    "                    f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
    "                    f\"\\t{evaluation_results['recall']}\"\n",
    "                    f\"\\t{evaluation_results['f-score']}\"\n",
    "                )\n",
    "\n",
    "    # Save model\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(\"model_artifacts\")\n",
    "\n",
    "\n",
    "def evaluate_model(tokenizer, textcat, test_data: list) -> dict:\n",
    "    reviews, labels = zip(*test_data)\n",
    "    reviews = (tokenizer(review) for review in reviews)\n",
    "    true_positives = 0\n",
    "    false_positives = 1e-8  # Can't be 0 because of presence in denominator\n",
    "    true_negatives = 0\n",
    "    false_negatives = 1e-8\n",
    "    for i, review in enumerate(textcat.pipe(reviews)):\n",
    "        true_label = labels[i][\"cats\"]\n",
    "        for predicted_label, score in review.cats.items():\n",
    "            # Every cats dictionary includes both labels, you can get all\n",
    "            # the info you need with just the pos label\n",
    "            if predicted_label == \"neg\":\n",
    "                continue\n",
    "            if score >= 0.5 and true_label[\"pos\"]:\n",
    "                true_positives += 1\n",
    "            elif score >= 0.5 and true_label[\"neg\"]:\n",
    "                false_positives += 1\n",
    "            elif score < 0.5 and true_label[\"neg\"]:\n",
    "                true_negatives += 1\n",
    "            elif score < 0.5 and true_label[\"pos\"]:\n",
    "                false_negatives += 1\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        f_score = 0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score}\n",
    "\n",
    "\n",
    "def test_model(input_data: str = TEST_REVIEW):\n",
    "    #  Load saved trained model\n",
    "    loaded_model = spacy.load(\"model_artifacts\")\n",
    "    # Generate prediction\n",
    "    parsed_text = loaded_model(input_data)\n",
    "    # Determine prediction to return\n",
    "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
    "        prediction = \"Positive\"\n",
    "        score = parsed_text.cats[\"pos\"]\n",
    "    else:\n",
    "        prediction = \"Negative\"\n",
    "        score = parsed_text.cats[\"neg\"]\n",
    "    print(\n",
    "        f\"Review text: {input_data}\\nPredicted sentiment: {prediction}\"\n",
    "        f\"\\tScore: {score}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_training_data(\n",
    "    data_directory: str = \"data/aclImdb/train\", split: float = 0.8, limit: int = 0\n",
    ") -> tuple:\n",
    "    # Load from files\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\") as f:\n",
    "                    text = f.read()\n",
    "                    text = text.replace(\"<br />\", \"\\n\\n\")\n",
    "                    if text.strip():\n",
    "                        spacy_label = {\n",
    "                            \"cats\": {\n",
    "                                \"pos\": \"pos\" == label,\n",
    "                                \"neg\": \"neg\" == label,\n",
    "                            }\n",
    "                        }\n",
    "                        reviews.append((text, spacy_label))\n",
    "    random.shuffle(reviews)\n",
    "\n",
    "    if limit:\n",
    "        reviews = reviews[:limit]\n",
    "    split = int(len(reviews) * split)\n",
    "    return reviews[:split], reviews[split:]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train, test = load_training_data(limit=25)\n",
    "    print(\"Training model\")\n",
    "    train_model(train, test)\n",
    "    df = pd.DataFrame(eval_list)\n",
    "    pd.DataFrame.plot(df)\n",
    "    print(\"Testing model\")\n",
    "    test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Beginning training\n",
      "Loss\tPrecision\tRecall\tF-score\n",
      "Training iteration 0\n",
      "0.18677512183785439\t0.3999999992\t0.999999995\t0.5714285697959184\n",
      "Training iteration 1\n",
      "0.22009881399571896\t0.0\t0.0\t0\n",
      "Training iteration 2\n",
      "0.15902768075466156\t0.3999999992\t0.999999995\t0.5714285697959184\n",
      "Training iteration 3\n",
      "0.1315029188990593\t0.9999999900000002\t0.4999999975\t0.6666666622222222\n",
      "Training iteration 4\n",
      "0.11989873833954334\t0.9999999900000002\t0.4999999975\t0.6666666622222222\n",
      "Training iteration 5\n",
      "0.05031450046226382\t0.6666666644444444\t0.999999995\t0.7999999968\n",
      "Training iteration 6\n",
      "0.019340375903993845\t0.9999999900000002\t0.4999999975\t0.6666666622222222\n",
      "Training iteration 7\n",
      "0.03598793293349445\t0.9999999900000002\t0.4999999975\t0.6666666622222222\n",
      "Training iteration 8\n",
      "0.013679868137842277\t0.999999995\t0.999999995\t0.999999995\n",
      "Training iteration 9\n",
      "0.003994399990915554\t0.9999999900000002\t0.4999999975\t0.6666666622222222\n",
      "Training iteration 10\n",
      "0.0003664554151328048\t0.9999999900000002\t0.4999999975\t0.6666666622222222\n",
      "Training iteration 11\n",
      "0.0006666773836059292\t0.999999995\t0.999999995\t0.999999995\n",
      "Training iteration 12\n",
      "2.808163628742477e-05\t0.999999995\t0.999999995\t0.999999995\n",
      "Training iteration 13\n",
      "0.0009733412689403309\t0.999999995\t0.999999995\t0.999999995\n",
      "Training iteration 14\n",
      "6.864118898874949e-05\t0.999999995\t0.999999995\t0.999999995\n",
      "Training iteration 15\n",
      "1.7573824038663588e-05\t0.999999995\t0.999999995\t0.999999995\n",
      "Training iteration 16\n",
      "7.262200927016238e-05\t0.999999995\t0.999999995\t0.999999995\n",
      "Training iteration 17\n",
      "5.535137566425874e-05\t0.999999995\t0.999999995\t0.999999995\n",
      "Training iteration 18\n",
      "1.8085634053477406e-05\t0.9999999900000002\t0.4999999975\t0.6666666622222222\n",
      "Training iteration 19\n",
      "5.355409916774079e-05\t0.9999999900000002\t0.4999999975\t0.6666666622222222\n",
      "Testing model\n",
      "Review text: \n",
      "Transcendently beautiful in moments outside the office, it seems almost\n",
      "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
      "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
      "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
      "doesn't matter. (The worst is sort of tedious - like Office Space with less\n",
      "humor.)\n",
      "\n",
      "Predicted sentiment: Negative\tScore: 0.9188067317008972\n",
      "Review text: \n",
      "Very Great Movie\n",
      "\n",
      "Predicted sentiment: Positive\tScore: 0.9999545812606812\n"
     ]
    }
   ],
   "source": [
    "#  source: https://www.rottentomatoes.com/m/the_little_things_2021/reviews?intcmp=rt-what-to-know_read-critics-reviews\n",
    "\n",
    "review_the_little_things_2021 = \"\"\"\n",
    "This thriller suffers from somewhat jarring plot turns, some overcooked performances, and other flaws, \n",
    "but the sturdy, classical direction and Washington's anguished performance make it worth seeing. \n",
    "\"\"\"\n",
    "\n",
    "review_the_little_things_2021_2 = \"\"\"\n",
    "Very Great Movie\n",
    "\"\"\"\n",
    "\n",
    "train, test = load_training_data(limit=25)\n",
    "print(\"Training model\")\n",
    "train_model(train, test)\n",
    "df = pd.DataFrame(eval_list)\n",
    "pd.DataFrame.plot(df)\n",
    "print(\"Testing model\")\n",
    "test_model(review_the_little_things_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: \n",
      "This thriller suffers from somewhat jarring plot turns, some overcooked performances, and other flaws, \n",
      "but the sturdy, classical direction and Washington's anguished performance make it worth seeing. \n",
      "\n",
      "Predicted sentiment: Negative\tScore: 0.7528043985366821\n"
     ]
    }
   ],
   "source": [
    "test_model(review_the_little_things_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

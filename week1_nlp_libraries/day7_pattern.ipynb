{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [1] https://stackabuse.com/python-for-nlp-introduction-to-the-pattern-library/\n",
    "- [2] [NLP Tutorial 3 - Extract Text from PDF Files in Python for NLP | PDF Writer and Reader in Python](https://youtu.be/_VSX7yd-zPE)\n",
    "- [3] https://analyticsindiamag.com/hands-on-guide-to-pattern-a-python-tool-for-effective-text-processing-and-data-mining/\n",
    "- [4] [General Comparison between different Python NLP Libraries](https://medium.com/towards-artificial-intelligence/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0)\n",
    "- [5] https://textminingonline.com/getting-started-with-pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "- The Pattern library is a multipurpose library capable of handling the following tasks: [1]\n",
    " - NLP: performing tasks such as tokenization, stemming, POS tagging, sentiment analysis, etc\n",
    " - Data Mining: has API to mine data from sites like Twitter, Facebook, Wikipedia, etc\n",
    " - ML: contains ML models such as SVM, KNN, and perceptron, which can be used for classification, regression, and clustering tasks\n",
    "- Even it's not as popular as spaCy or NLTK, it has unique functionalities such as finding superlatives and comparatives, get fact and opinion detecetion which other NLP libraries doesn't have [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pattern\n",
      "  Downloading Pattern-3.6.0.tar.gz (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 18.8 MB/s eta 0:00:01   |██████████▎                     | 7.1 MB 2.0 MB/s eta 0:00:08     |██████████████████████████▌     | 18.5 MB 18.8 MB/s eta 0:00:01     |███████████████████████████████ | 21.6 MB 18.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pattern) (0.17.1)\n",
      "Collecting backports.csv\n",
      "  Downloading backports.csv-1.0.7-py2.py3-none-any.whl (12 kB)\n",
      "Collecting mysqlclient\n",
      "  Downloading mysqlclient-2.0.3.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pattern) (4.9.0)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pattern) (4.6.2)\n",
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.2-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 11.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pdfminer.six\n",
      "  Downloading pdfminer.six-20201018-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 9.6 MB/s eta 0:00:01     |███▌                            | 604 kB 9.6 MB/s eta 0:00:01     |█████████▋                      | 1.7 MB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pattern) (1.18.5)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pattern) (1.4.1)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pattern) (3.5)\n",
      "Collecting python-docx\n",
      "  Downloading python-docx-0.8.10.tar.gz (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cherrypy\n",
      "  Downloading CherryPy-18.6.0-py2.py3-none-any.whl (419 kB)\n",
      "\u001b[K     |████████████████████████████████| 419 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pattern) (2.23.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from beautifulsoup4->pattern) (2.0)\n",
      "Collecting jaraco.collections\n",
      "  Downloading jaraco.collections-3.1.0-py3-none-any.whl (9.9 kB)\n",
      "Collecting cheroot>=8.2.1\n",
      "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: more-itertools in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from cherrypy->pattern) (8.2.0)\n",
      "Collecting zc.lockfile\n",
      "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting portend>=2.1.1\n",
      "  Downloading portend-2.7.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting jaraco.functools\n",
      "  Downloading jaraco.functools-3.1.0-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
      "Collecting tempora>=1.8\n",
      "  Downloading tempora-4.0.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2019.3)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Collecting jaraco.text\n",
      "  Downloading jaraco.text-3.4.0-py3-none-any.whl (8.3 kB)\n",
      "Collecting jaraco.classes\n",
      "  Downloading jaraco.classes-3.2.0-py3-none-any.whl (5.6 kB)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.1.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.4 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nltk->pattern) (4.46.0)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nltk->pattern) (2020.11.13)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nltk->pattern) (0.11)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from nltk->pattern) (7.1.1)\n",
      "Requirement already satisfied: chardet in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pdfminer.six->pattern) (3.0.4)\n",
      "Requirement already satisfied: cryptography in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pdfminer.six->pattern) (3.1.1)\n",
      "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pdfminer.six->pattern) (2.1.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from cryptography->pdfminer.six->pattern) (1.14.3)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six->pattern) (2.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests->pattern) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests->pattern) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from requests->pattern) (2.9)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from zc.lockfile->cherrypy->pattern) (46.1.3)\n",
      "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
      "  Building wheel for pattern (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pattern: filename=Pattern-3.6-py3-none-any.whl size=22332722 sha256=c57d78b30fa8f5b5b54948d5be2c76a029ab4241af1f6e35f1d10e02012363e3\n",
      "  Stored in directory: /Users/enlik/Library/Caches/pip/wheels/d1/ac/72/67cc168ba463decbc0ca253e0af88da90a39a630432063f145\n",
      "  Building wheel for mysqlclient (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mysqlclient: filename=mysqlclient-2.0.3-cp36-cp36m-macosx_10_9_x86_64.whl size=55901 sha256=842fcc1c259db3cb49a318ff530e2c8f714e43df84d0d460207ccaba9f0afdfe\n",
      "  Stored in directory: /Users/enlik/Library/Caches/pip/wheels/26/d0/50/9ab7ee785bf3c7485d19fe645ea080478c54a16ab81c771112\n",
      "  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.10-py3-none-any.whl size=184489 sha256=303a188e984e55b31c74b4a4cb9fe842f9800c5b01227a8a93bc0fb8a3584688\n",
      "  Stored in directory: /Users/enlik/Library/Caches/pip/wheels/83/84/21/ca046018e83edef96581a58f9dde2ac3b0d2919d624f0663d9\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6064 sha256=d17355678220c331bce519bdb2c31121e55bd734176e1ea6a445d2eae990edfb\n",
      "  Stored in directory: /Users/enlik/Library/Caches/pip/wheels/bb/cb/26/83c0b63161dc478ded6db8d83a148d32b6cce8606043c2023b\n",
      "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
      "Installing collected packages: jaraco.functools, importlib-resources, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cheroot, python-docx, pdfminer.six, mysqlclient, feedparser, cherrypy, backports.csv, pattern\n",
      "Successfully installed backports.csv-1.0.7 cheroot-8.5.2 cherrypy-18.6.0 feedparser-6.0.2 importlib-resources-5.1.0 jaraco.classes-3.2.0 jaraco.collections-3.1.0 jaraco.functools-3.1.0 jaraco.text-3.4.0 mysqlclient-2.0.3 pattern-3.6 pdfminer.six-20201018 portend-2.7.0 python-docx-0.8.10 sgmllib3k-1.0.0 tempora-4.0.1 zc.lockfile-2.0\n"
     ]
    }
   ],
   "source": [
    "## installation\n",
    "# !pip install pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for NLP: Introduction to the Pattern Library [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Library Functions for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing, POS Tagging, and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import parse\n",
    "from pattern.en import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          WORD   TAG    CHUNK   ROLE   ID     PNP    LEMMA       \n",
      "                                                                 \n",
      "             I   PRP    NP      SBJ    1      -      i           \n",
      "         drove   VBD    VP      -      1      -      drive       \n",
      "            my   PRP$   NP      OBJ    1      -      my          \n",
      "           car   NN     NP ^    OBJ    1      -      car         \n",
      "            to   TO     -       -      -      -      to          \n",
      "           the   DT     NP      -      -      -      the         \n",
      "      hospital   NN     NP ^    -      -      -      hospital    \n",
      "     yesterday   NN     NP ^    -      -      -      yesterday   \n"
     ]
    }
   ],
   "source": [
    "pprint(parse('I drove my car to the hospital yesterday', relations=True, lemmata=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['I', 'PRP', 'B-NP', 'O', 'NP-SBJ-1', 'i'], ['drove', 'VBD', 'B-VP', 'O', 'VP-1', 'drive'], ['my', 'PRP$', 'B-NP', 'O', 'NP-OBJ-1', 'my'], ['car', 'NN', 'I-NP', 'O', 'NP-OBJ-1', 'car'], ['to', 'TO', 'O', 'O', 'O', 'to'], ['the', 'DT', 'B-NP', 'O', 'O', 'the'], ['hospital', 'NN', 'I-NP', 'O', 'O', 'hospital'], ['yesterday', 'NN', 'I-NP', 'O', 'O', 'yesterday']]]\n"
     ]
    }
   ],
   "source": [
    "print(parse('I drove my car to the hospital yesterday', relations=True, lemmata=True).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pluralizing and Singularizing the Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leaves\n",
      "theife\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import pluralize, singularize\n",
    "\n",
    "print(pluralize('leaf'))\n",
    "print(singularize('theives'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Adjective to Comparative and Superlative Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "best\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import comparative, superlative\n",
    "\n",
    "print(comparative('good'))\n",
    "print(superlative('good'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'goes'), ('goes', 'to'), ('to', 'hospital')]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import ngrams\n",
    "\n",
    "print(ngrams(\"He goes to hospital\", n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.75, 0.8)\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import sentiment\n",
    "\n",
    "print(sentiment(\"This is an excellent movie to watch. I really love it\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- 0.75 show the sentiment score of the sentence that means highly positive\n",
    "- 0.8 is the subjectivity score that is a personal of the user  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if a Statement is a Fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import parse, Sentence\n",
    "from pattern.en import modality\n",
    "\n",
    "text = \"Paris is the capital of France\"\n",
    "sent = parse(text, lemmata=True)\n",
    "sent = Sentence(sent)\n",
    "\n",
    "print(modality(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "text = \"I think we can complete this task\"\n",
    "sent = parse(text, lemmata=True)\n",
    "sent = Sentence(sent)\n",
    "\n",
    "print(modality(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('While', 0.6459209419680404), ('White', 0.2968881412952061), ('Title', 0.03280067283431455), ('Whistle', 0.023549201009251473), ('Chile', 0.0008410428931875525)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "\n",
    "print(suggest(\"Whitle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fracture', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import suggest\n",
    "print(suggest(\"Fracture\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "two hundred and fifty-six point thirty-nine\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import number, numerals\n",
    "\n",
    "print(number(\"one hundred and twenty two\"))\n",
    "print(numerals(256.390, round=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "several bananas, several apples and a pair of mangoes\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import quantify\n",
    "\n",
    "print(quantify(['apple', 'apple', 'apple', 'banana', 'banana', 'banana', 'mango', 'mango']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hundreds of strawberries and a number of peaches\n",
      "thousands of oranges\n"
     ]
    }
   ],
   "source": [
    "from pattern.en import quantify\n",
    "\n",
    "print(quantify({'strawberry': 200, 'peach': 15}))\n",
    "print(quantify('orange', amount=1200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Library Functions for Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For macOS SSL issue when downloading file(s) from external sources\n",
    "import ssl \n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Web Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.web import download\n",
    "\n",
    "page_html = download('https://en.wikipedia.org/wiki/Artificial_intelligence', unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.web import URL, extension\n",
    "\n",
    "page_url = URL('https://upload.wikimedia.org/wikipedia/commons/f/f1/RougeOr_football.jpg')\n",
    "file = open('football' + extension(page_url.page), 'wb')\n",
    "file.write(page_url.download())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding URLs within Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['www.google.com']\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import find_urls\n",
    "\n",
    "print(find_urls('To search anything, go to www.google.com', unique=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Asynchronous Requests for Webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "searching...\n",
      "[Result({'url': 'https://en.wikipedia.org/wiki/Artificial_intelligence', 'title': 'Artificial intelligence - Wikipedia', 'text': '<b>Artificial intelligence</b> (<b>AI</b>) is intelligence demonstrated by machines, unlike the <br>\\nnatural intelligence displayed by humans and animals, which involves&nbsp;...'}), Result({'url': 'https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp', 'title': 'Artificial Intelligence (AI) Definition', 'text': '... <b>Artificial intelligence</b> (<b>AI</b>) refers to the simulation of human intelligence in <br>\\nmachines that are programmed to think like humans and mimic their&nbsp;...', 'date': 'Jan 6, 2021'}), Result({'url': 'https://builtin.com/artificial-intelligence', 'title': 'What is Artificial Intelligence? How Does AI Work? | Built In', 'text': '<b>Artificial intelligence</b> (<b>AI</b>) is wide-ranging branch of computer science concerned <br>\\nwith building smart machines capable of performing tasks that typically require&nbsp;...'}), Result({'url': 'https://www.aaai.org/', 'title': 'Association for the Advancement of Artificial Intelligence', 'text': 'AAAI advances the understanding of the mechanisms underlying thought and <br>\\n<b>intelligent</b> behavior and their embodiment in machines.'}), Result({'url': 'https://www.britannica.com/technology/artificial-intelligence', 'title': 'artificial intelligence | Definition, Examples, and Applications ...', 'text': '<b>Artificial intelligence</b>, the ability of a computer or computer-controlled robot to <br>\\nperform tasks commonly associated with intelligent beings. The term is frequently<br>\\n&nbsp;...'}), Result({'url': 'https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/', 'title': 'Benefits & Risks of Artificial Intelligence - Future of Life Institute', 'text': 'What is <b>AI</b>? From SIRI to self-driving cars, <b>artificial intelligence</b> (<b>AI</b>) is progressing <br>\\nrapidly. While science fiction often&nbsp;...'}), Result({'url': 'https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html', 'title': 'Artificial Intelligence – What it is and why it matters | SAS', 'text': '<b>Artificial intelligence</b> (<b>AI</b>) makes it possible for machines to learn from experience, <br>\\nadjust to new inputs and perform human-like tasks. Most <b>AI</b> examples that you&nbsp;...'}), Result({'url': 'https://www.brookings.edu/research/what-is-artificial-intelligence/', 'title': 'What is artificial intelligence?', 'text': '... <b>Artificial intelligence</b> algorithms are designed to make decisions, often using real-<br>\\ntime data. They are unlike passive machines that are capable&nbsp;...', 'date': 'Oct 4, 2018'}), Result({'url': 'https://www.zdnet.com/article/what-is-ai-everything-you-need-to-know-about-artificial-intelligence/', 'title': 'What is AI? Everything you need to know about Artificial Intelligence ...', 'text': '... What is <b>artificial intelligence</b> (<b>AI</b>)?. It depends who you ask. Back in the 1950s, the <br>\\nfathers of the field, Minsky and McCarthy, described artificial&nbsp;...', 'date': 'Dec 11, 2020'}), Result({'url': 'https://www.journals.elsevier.com/artificial-intelligence', 'title': 'Artificial Intelligence - Journal - Elsevier', 'text': 'The journal of <b>Artificial Intelligence</b> (AIJ) welcomes papers on broad aspects of <b>AI</b> <br>\\nthat constitute advances in the overall field including, but not...'})]\n",
      "['https://en.wikipedia.org/wiki/Artificial_intelligence', 'https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp', 'https://builtin.com/artificial-intelligence', 'https://www.aaai.org/', 'https://www.britannica.com/technology/artificial-intelligence', 'https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/', 'https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html', 'https://www.brookings.edu/research/what-is-artificial-intelligence/', 'https://www.zdnet.com/article/what-is-ai-everything-you-need-to-know-about-artificial-intelligence/', 'https://www.journals.elsevier.com/artificial-intelligence']\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import asynchronous, time, Google\n",
    "\n",
    "asyn_req = asynchronous(Google().search, 'artificial intelligence', timeout=4)\n",
    "while not asyn_req.done:\n",
    "    time.sleep(0.1)\n",
    "    print('searching...')\n",
    "\n",
    "print(asyn_req.value)\n",
    "\n",
    "print(find_urls(asyn_req.value, unique=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Search Engine Results with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Artificial_intelligence\n",
      "<b>Artificial intelligence</b> (<b>AI</b>) is intelligence demonstrated by machines, unlike the <br>\n",
      "natural intelligence displayed by humans and animals, which involves&nbsp;...\n",
      "https://www.investopedia.com/terms/a/artificial-intelligence-ai.asp\n",
      "... <b>Artificial intelligence</b> (<b>AI</b>) refers to the simulation of human intelligence in <br>\n",
      "machines that are programmed to think like humans and mimic their&nbsp;...\n",
      "https://builtin.com/artificial-intelligence\n",
      "<b>Artificial intelligence</b> (<b>AI</b>) is wide-ranging branch of computer science concerned <br>\n",
      "with building smart machines capable of performing tasks that typically require&nbsp;...\n",
      "https://www.aaai.org/\n",
      "AAAI advances the understanding of the mechanisms underlying thought and <br>\n",
      "<b>intelligent</b> behavior and their embodiment in machines.\n",
      "https://www.britannica.com/technology/artificial-intelligence\n",
      "<b>Artificial intelligence</b>, the ability of a computer or computer-controlled robot to <br>\n",
      "perform tasks commonly associated with intelligent beings. The term is frequently<br>\n",
      "&nbsp;...\n",
      "https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/\n",
      "What is <b>AI</b>? From SIRI to self-driving cars, <b>artificial intelligence</b> (<b>AI</b>) is progressing <br>\n",
      "rapidly. While science fiction often&nbsp;...\n",
      "https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html\n",
      "<b>Artificial intelligence</b> (<b>AI</b>) makes it possible for machines to learn from experience, <br>\n",
      "adjust to new inputs and perform human-like tasks. Most <b>AI</b> examples that you&nbsp;...\n",
      "https://www.brookings.edu/research/what-is-artificial-intelligence/\n",
      "... <b>Artificial intelligence</b> algorithms are designed to make decisions, often using real-<br>\n",
      "time data. They are unlike passive machines that are capable&nbsp;...\n",
      "https://www.zdnet.com/article/what-is-ai-everything-you-need-to-know-about-artificial-intelligence/\n",
      "... What is <b>artificial intelligence</b> (<b>AI</b>)?. It depends who you ask. Back in the 1950s, the <br>\n",
      "fathers of the field, Minsky and McCarthy, described artificial&nbsp;...\n",
      "https://www.journals.elsevier.com/artificial-intelligence\n",
      "The journal of <b>Artificial Intelligence</b> (AIJ) welcomes papers on broad aspects of <b>AI</b> <br>\n",
      "that constitute advances in the overall field including, but not...\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import Google\n",
    "\n",
    "google = Google(license=None)\n",
    "for search_result in google.search('artificial intelligence'):\n",
    "    print(search_result.url)\n",
    "    print(search_result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Stevewal63: Artificial Intelligence Will Change The Way We Work Once We Get Back To The Office https://t.co/WmjSi4yt4B\n",
      "RT @TheNextTech2018: 8 Tips to Use Artificial Intelligence (AI) in Mobile Apps\n",
      "\n",
      "Read post: - https://t.co/HXoH8DeI5N\n",
      "\n",
      "#artificialintelligence #mobileapps #artificialintelligenceai #tips #tip #intelligence #mobile #apps #illustration #fiction #animation #art #cartoons https://t.co/yOiUDnRuFR\n",
      "RT @PhathaATM: @somadodafikeni Not at the time😅 until I discovered that actually the whole machine learning and to a larger extent the artificial intelligence knowledge depends on this understanding. Unbelievable!\n",
      "RT @KevinClarity: “Automating Trading and Market Making With Artificial Intelligence” by @PoseysThumbs\n",
      "https://t.co/bK0tcydBg9\n",
      "\n",
      "#Machinelearning #100DaysOfCode #IoT #IIoT #Bigdata #100DaysOfMLCode #Python #flutter #cybersecurity #RStats #CodeNewbie #DataScience #DEVCommunity #RPA\n",
      "RT @STPIBHOPAL: The use of data analytics, artificial intelligence, machine learning to make regulatory filings is more frictionless for businesses and startups. #Budget2021 #AatmanirbharBharatKaBudget @rsprasad   @SanjayDhotreMP   @Omkar_Raii https://t.co/Mp6Qfvdqog\n",
      "RT @KevinClarity: “Automating Trading and Market Making With Artificial Intelligence” by @PoseysThumbs\n",
      "https://t.co/bK0tcydBg9\n",
      "\n",
      "#Machinelearning #100DaysOfCode #IoT #IIoT #Bigdata #100DaysOfMLCode #Python #flutter #cybersecurity #RStats #CodeNewbie #DataScience #DEVCommunity #RPA\n",
      "RT @KevinClarity: “Automating Trading and Market Making With Artificial Intelligence” by @PoseysThumbs\n",
      "https://t.co/bK0tcydBg9\n",
      "\n",
      "#Machinelearning #100DaysOfCode #IoT #IIoT #Bigdata #100DaysOfMLCode #Python #flutter #cybersecurity #RStats #CodeNewbie #DataScience #DEVCommunity #RPA\n",
      "UOL - PGDip in Data Science and Artificial Intelligence [Europe] https://t.co/kc9pYAbmTY #datascience\n",
      "RT @rsprasad: भारत में ease of justice और भी तेजी से बढ़े इस दिशा में सुप्रीम कोर्ट की e-Committee,@NICMeity के साथ मिलकर काम कर रही है। हमारे justice system को future ready बनाने के लिए न्याय प्रक्रियाओं में Artificial Intelligence के इस्तेमाल की संभावना को भी तलाशा जा रहा है:PM @narendramodi https://t.co/dLJm5YPvw2\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "index = None\n",
    "for j in range(3):\n",
    "    for tweet in twitter.search('artificial intelligence', start=index, count=3):\n",
    "        print(tweet.text)\n",
    "        index = tweet.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting HTML Data to Plain Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python for NLP: Introduction to the TextBlob Library\n",
      "\n",
      "Toggle navigation Stack Abuse\n",
      "\n",
      "* JavaScript\n",
      "* Python\n",
      "* Java\n",
      "* Jobs\n",
      "\n",
      "Python for NLP: Introduction to the TextBlob Library\n",
      "\n",
      "By\n",
      "\n",
      "Usman Malik\n",
      "\n",
      "•0 Comments\n",
      "\n",
      "Introduction\n",
      "\n",
      "This is the seventh article in my series of articles on Python for NLP. In my previous article, I explained how to perform topic modeling using Latent Dirichlet Allocation and Non-Negative Matrix factorization. We used the Scikit-Learn library to perform topic modeling.\n",
      "\n",
      "In this article, we will explore TextBlob, which is another extremely powerful NLP library for Python. TextBlob is built upon NLTK and provides an easy to use interface to the NLTK library. We will see how TextBlob can be used to perform a variety of NLP tasks ranging from parts-of-speech tagging to sentiment analysis, and language translation to text classification.\n",
      "\n",
      "The detailed download instructions for the library can be found at the official link. I would suggest that you install the TextBlob library as well as the sample corpora.\n",
      "\n",
      "Here is the gist of the instructions linked above, but be sure to check the official documentation for more instructions on installing if you need it:\n",
      "\n",
      "$ pip install -U textblob\n",
      "\n",
      "And to install the corpora:\n",
      "\n",
      "$ python -m textblob.download_corpora\n",
      "\n",
      "Let's now see the different functionalities of the TextBlob library.\n",
      "\n",
      "Tokenization\n",
      "\n",
      "Tokenization refers to splitting a large paragraph into sentences or words. Typically, a token refers to a word in a text document. Tokenization is pretty straight forward with TextBlob. All you have to do is import the\n",
      "TextBlob\n",
      "\n",
      "object from the\n",
      "textblob\n",
      "\n",
      "library, pass it the document that you want to tokenize, and then use the\n",
      "sentences\n",
      "\n",
      "and\n",
      "words\n",
      "\n",
      "attributes to get the tokenized sentences and attributes. Let's see this in action:\n",
      "\n",
      "The first step is to import the\n",
      "TextBlob\n",
      "\n",
      "object:\n",
      "\n",
      "from textblob import TextBlob\n",
      "\n",
      "Next, you need to define a string that contains the text of the document. We will create string that contains the first paragraph of the Wikipedia article on artificial intelligence.\n",
      "\n",
      "document = (\"In computer science, artificial intelligence (AI), \\\n",
      "sometimes called machine intelligence, is intelligence \\\n",
      "demonstrated by machines, in contrast to the natural intelligence \\\n",
      "displayed by humans and animals. Computer science defines AI \\\n",
      "research as the study of \\\"intelligent agents\\\": any device that \\\n",
      "perceives its environment and takes actions that maximize its\\\n",
      "chance of successfully achieving its goals.[1] Colloquially,\\\n",
      "the term \\\"artificial intelligence\\\" is used to describe machines\\\n",
      "that mimic \\\"cognitive\\\" functions that humans associate with other\\\n",
      "human minds, such as \\\"learning\\\" and \\\"problem solving\\\".[2]\")\n",
      "\n",
      "The next step is to pass this document as a parameter to the\n",
      "TextBlob\n",
      "\n",
      "class. The returned object can then be used to tokenize the document to words and sentences.\n",
      "\n",
      "text_blob_object = TextBlob(document)\n",
      "\n",
      "Now to get the tokenized sentences, we can use the\n",
      "sentences\n",
      "\n",
      "attribute:\n",
      "\n",
      "document_sentence = text_blob_object.sentences\n",
      "\n",
      "print(document_sentence)\n",
      "print(len(document_sentence))\n",
      "\n",
      "In the output, you will see the tokenized sentences along with the number of sentences.\n",
      "\n",
      "[Sentence(\"In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals.\"), Sentence(\"Computer science defines AI research as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\"), Sentence(\"[1] Colloquially, the term \"artificial intelligence\" is used to describe machines that mimic \"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".\"), Sentence(\"[2]\")]\n",
      "4\n",
      "\n",
      "Similarly, the\n",
      "words\n",
      "\n",
      "attribute returns the tokenized words in the document.\n",
      "\n",
      "document_words = text_blob_object.words\n",
      "\n",
      "print(document_words)\n",
      "print(len(document_words))\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "['In', 'computer', 'science', 'artificial', 'intelligence', 'AI', 'sometimes', 'called', 'machine', 'intelligence', 'is', 'intelligence', 'demonstrated', 'by', 'machines', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'displayed', 'by', 'humans', 'and', 'animals', 'Computer', 'science', 'defines', 'AI', 'research', 'as', 'the', 'study', 'of', 'intelligent', 'agents', 'any', 'device', 'that', 'perceives', 'its', 'environment', 'and', 'takes', 'actions', 'that', 'maximize', 'its', 'chance', 'of', 'successfully', 'achieving', 'its', 'goals', '1', 'Colloquially', 'the', 'term', 'artificial', 'intelligence', 'is', 'used', 'to', 'describe', 'machines', 'that', 'mimic', 'cognitive', 'functions', 'that', 'humans', 'associate', 'with', 'other', 'human', 'minds', 'such', 'as', 'learning', 'and', 'problem', 'solving', '2']\n",
      "84\n",
      "\n",
      "Lemmatization\n",
      "\n",
      "Lemmatization refers to reducing the word to its root form as found in a dictionary.\n",
      "\n",
      "To perform lemmatization via TextBlob, you have to use the\n",
      "Word\n",
      "\n",
      "object from the\n",
      "textblob\n",
      "\n",
      "library, pass it the word that you want to lemmatize and then call the\n",
      "lemmatize\n",
      "\n",
      "method.\n",
      "\n",
      "from textblob import Word\n",
      "\n",
      "word1 = Word(\"apples\")\n",
      "print(\"apples:\", word1.lemmatize())\n",
      "\n",
      "word2 = Word(\"media\")\n",
      "print(\"media:\", word2.lemmatize())\n",
      "\n",
      "word3 = Word(\"greater\")\n",
      "print(\"greater:\", word3.lemmatize(\"a\"))\n",
      "\n",
      "In the script above, we perform lemmatization on the words \"apples\", \"media\", and \"greater\". In the output, you will see the words \"apple\", (which is singular for the apple), \"medium\" (which is singular for the medium) and \"great\" (which is the positive degree for the word greater). Notice that for the word greater, we pass \"a\" as a parameter to the\n",
      "lemmatize\n",
      "\n",
      "method. This specifically tells the method that the word should be treated as an adjective. By default, the words are treated as nouns by the\n",
      "lemmatize()\n",
      "\n",
      "method. The complete list for the parts of speech components is as follows:\n",
      "\n",
      "ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
      "\n",
      "Parts of Speech (POS) Tagging\n",
      "\n",
      "Like the spaCy and NLTK libraries, the TextBlob library also contains functionalities for the POS tagging.\n",
      "\n",
      "To find POS tags for the words in a document, all you have to do is use the\n",
      "tags\n",
      "\n",
      "attribute as shown below:\n",
      "\n",
      "for word, pos in text_blob_object.tags:\n",
      "print(word + \" => \" + pos)\n",
      "\n",
      "In the script above, print the tags for all the words in the first paragraph of the Wikipedia article on Artificial Intelligence. The output of the script above looks like this:\n",
      "\n",
      "```\n",
      "In => IN\n",
      "computer => NN\n",
      "science => NN\n",
      "artificial => JJ\n",
      "intelligence => NN\n",
      "AI => NNP\n",
      "sometimes => RB\n",
      "called => VBD\n",
      "machine => NN\n",
      "intelligence => NN\n",
      "is => VBZ\n",
      "intelligence => NN\n",
      "demonstrated => VBN\n",
      "by => IN\n",
      "machines => NNS\n",
      "in => IN\n",
      "contrast => NN\n",
      "to => TO\n",
      "the => DT\n",
      "natural => JJ\n",
      "intelligence => NN\n",
      "displayed => VBN\n",
      "by => IN\n",
      "humans => NNS\n",
      "and => CC\n",
      "animals => NNS\n",
      "Computer => NNP\n",
      "science => NN\n",
      "defines => NNS\n",
      "AI => NNP\n",
      "research => NN\n",
      "as => IN\n",
      "the => DT\n",
      "study => NN\n",
      "of => IN\n",
      "intelligent => JJ\n",
      "agents => NNS\n",
      "any => DT\n",
      "device => NN\n",
      "that => WDT\n",
      "perceives => VBZ\n",
      "its => PRP$\n",
      "environment => NN\n",
      "and => CC\n",
      "takes => VBZ\n",
      "actions => NNS\n",
      "that => IN\n",
      "maximize => VB\n",
      "its => PRP$\n",
      "chance => NN\n",
      "of => IN\n",
      "successfully => RB\n",
      "achieving => VBG\n",
      "its => PRP$\n",
      "goals => NNS\n",
      "[ => RB\n",
      "1 => CD\n",
      "] => NNP\n",
      "Colloquially => NNP\n",
      "the => DT\n",
      "term => NN\n",
      "artificial => JJ\n",
      "intelligence => NN\n",
      "is => VBZ\n",
      "used => VBN\n",
      "to => TO\n",
      "describe => VB\n",
      "machines => NNS\n",
      "that => IN\n",
      "mimic => JJ\n",
      "cognitive => JJ\n",
      "functions => NNS\n",
      "that => WDT\n",
      "humans => NNS\n",
      "associate => VBP\n",
      "with => IN\n",
      "other => JJ\n",
      "human => JJ\n",
      "minds => NNS\n",
      "such => JJ\n",
      "as => IN\n",
      "learning => VBG\n",
      "and => CC\n",
      "problem => NN\n",
      "solving => NN\n",
      "[ => RB\n",
      "2 => CD\n",
      "] => NNS\n",
      "```\n",
      "\n",
      "The POS tags have been printed in the abbreviation form. To see the full form of each abbreviation, please consult this link.\n",
      "\n",
      "Convert Text to Singular and Plural\n",
      "\n",
      "TextBlob also allows you to convert text into a plural or singular form using the\n",
      "pluralize\n",
      "\n",
      "and\n",
      "singularize\n",
      "\n",
      "methods, respectively. Look at the following example:\n",
      "\n",
      "text = (\"Football is a good game. It has many health benefit\")\n",
      "text_blob_object = TextBlob(text)\n",
      "print(text_blob_object.words.pluralize())\n",
      "\n",
      "In the output, you will see the plural of all the words:\n",
      "\n",
      "['Footballs', 'iss', 'some', 'goods', 'games', 'Its', 'hass', 'manies', 'healths', 'benefits']\n",
      "\n",
      "Similarly, to singularize words you can use\n",
      "singularize\n",
      "\n",
      "method as follows:\n",
      "\n",
      "text = (\"Footballs is a goods games. Its has many healths benefits\")\n",
      "\n",
      "text_blob_object = TextBlob(text)\n",
      "print(text_blob_object.words.singularize())\n",
      "\n",
      "The output of the script above looks like this:\n",
      "\n",
      "['Football', 'is', 'a', 'good', 'game', 'It', 'ha', 'many', 'health', 'benefit']\n",
      "\n",
      "Noun Phrase Extraction\n",
      "\n",
      "Noun phrase extraction, as the name suggests, refers to extracting phrases that contain nouns. Let's find all the noun phrases in the first paragraph of the Wikipedia article on artificial intelligence that we used earlier.\n",
      "\n",
      "To find noun phrases, you simply have to use the\n",
      "noun_phrase\n",
      "\n",
      "attributes on the\n",
      "TextBlob\n",
      "\n",
      "object. Look at the following example:\n",
      "\n",
      "text_blob_object = TextBlob(document)\n",
      "for noun_phrase in text_blob_object.noun_phrases:\n",
      "print(noun_phrase)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "computer science\n",
      "artificial intelligence\n",
      "ai\n",
      "machine intelligence\n",
      "natural intelligence\n",
      "computer\n",
      "science defines\n",
      "ai\n",
      "intelligent agents\n",
      "colloquially\n",
      "artificial intelligence\n",
      "describe machines\n",
      "human minds\n",
      "\n",
      "You can see all the noun phrases in our document.\n",
      "\n",
      "Getting Words and Phrase Counts\n",
      "\n",
      "In a previous section, we used Python's built-in\n",
      "len\n",
      "\n",
      "method to count the number of sentences, words and noun-phrases returned by the\n",
      "TextBlob\n",
      "\n",
      "object. We can use TextBlob's built-in methods for the same purpose.\n",
      "\n",
      "To find the frequency of occurrence of a particular word, we have to pass the name of the word as the index to the\n",
      "word_counts\n",
      "\n",
      "list of the\n",
      "TextBlob\n",
      "\n",
      "object.\n",
      "\n",
      "In the following example, we will count the number of instances of the word \"intelligence\" in the first paragraph of the Wikipedia article on Artificial Intelligence.\n",
      "\n",
      "text_blob_object = TextBlob(document)\n",
      "text_blob_object.word_counts['intelligence']\n",
      "\n",
      "Another way is to simply call the\n",
      "count\n",
      "\n",
      "method on the\n",
      "words\n",
      "\n",
      "attribute, and pass the name of the word whose frequency of occurrence is to be found as shown below:\n",
      "\n",
      "text_blob_object.words.count('intelligence')\n",
      "\n",
      "It is important to mention that by default the search is not case-sensitive. If you want your search to be case sensitive, you need to pass\n",
      "True\n",
      "\n",
      "as the value for the\n",
      "case_sensitive\n",
      "\n",
      "parameter, as shown below:\n",
      "\n",
      "text_blob_object.words.count('intelligence', case_sensitive=True)\n",
      "\n",
      "Like word counts, noun phrases can also be counted in the same way. The following example finds the phrase \"artificial intelligence\" in the paragraph.\n",
      "\n",
      "text_blob_object = TextBlob(document)\n",
      "text_blob_object.noun_phrases.count('artificial intelligence')\n",
      "\n",
      "In the output, you will see 2.\n",
      "\n",
      "Converting to Upper and Lowercase\n",
      "\n",
      "TextBlob objects are very similar to strings. You can convert them to upper case or lower case, change their values, and concatenate them together as well. In the following script, we convert the text from the TextBlob object to upper case:\n",
      "\n",
      "text = \"I love to watch football, but I have never played it\"\n",
      "text_blob_object = TextBlob(text)\n",
      "\n",
      "print(text_blob_object.upper())\n",
      "\n",
      "In the output, you will the string in the upper case:\n",
      "\n",
      "I LOVE TO WATCH FOOTBALL, BUT I HAVE NEVER PLAYED IT\n",
      "\n",
      "Similarly to convert the text to lowercase, we can use the\n",
      "lower()\n",
      "\n",
      "method as shown below:\n",
      "\n",
      "text = \"I LOVE TO WATCH FOOTBALL, BUT I HAVE NEVER PLAYED IT\"\n",
      "text_blob_object = TextBlob(text)\n",
      "\n",
      "print(text_blob_object.lower())\n",
      "\n",
      "Finding N-Grams\n",
      "\n",
      "N-Grams refer to n combination of words in a sentence. For instance, for a sentence \"I love watching football\", some 2-grams would be (I love), (love watching) and (watching football). N-Grams can play a crucial role in text classification.\n",
      "\n",
      "In TextBlob, N-grams can be found by passing the number of N-Grams to the\n",
      "ngrams\n",
      "\n",
      "method of the\n",
      "TextBlob\n",
      "\n",
      "object. Look at the following example:\n",
      "\n",
      "text = \"I love to watch football, but I have never played it\"\n",
      "text_blob_object = TextBlob(text)\n",
      "for ngram in text_blob_object.ngrams(2):\n",
      "print(ngram)\n",
      "\n",
      "The output of the script looks like this:\n",
      "\n",
      "['I', 'love']\n",
      "['love', 'to']\n",
      "['to', 'watch']\n",
      "['watch', 'football']\n",
      "['football', 'but']\n",
      "['but', 'I']\n",
      "['I', 'have']\n",
      "['have', 'never']\n",
      "['never', 'played']\n",
      "['played', 'it']\n",
      "\n",
      "This is especially helpful when training language models or doing any type of text prediction.\n",
      "\n",
      "Spelling Corrections\n",
      "\n",
      "Spelling correction is one of the unique functionalities of the TextBlob library. With the\n",
      "correct\n",
      "\n",
      "method of the\n",
      "TextBlob\n",
      "\n",
      "object, you can correct all the spelling mistakes in your text. Look at the following example:\n",
      "\n",
      "text = \"I love to watchf footbal, but I have neter played it\"\n",
      "text_blob_object = TextBlob(text)\n",
      "\n",
      "print(text_blob_object.correct())\n",
      "\n",
      "In the script above we made three spelling mistakes: \"watchf\" instead of \"watch\", \"footbal\" instead of \"football\", \"neter\" instead of \"never\". In the output, you will see that these mistakes have been corrected by TextBlob, as shown below:\n",
      "\n",
      "I love to watch football, but I have never played it\n",
      "\n",
      "Language Translation\n",
      "\n",
      "One of the most powerful capabilities of the TextBlob library is to translate from one language to another. On the backend, the TextBlob language translator uses the Google Translate API\n",
      "\n",
      "To translate from one language to another, you simply have to pass the text to the\n",
      "TextBlob\n",
      "\n",
      "object and then call the\n",
      "translate\n",
      "\n",
      "method on the object. The language code for the language that you want your text to be translated to is passed as a parameter to the method. Let's take a look at an example:\n",
      "\n",
      "text_blob_object_french = TextBlob(u'Salut comment allez-vous?')\n",
      "print(text_blob_object_french.translate(to='en'))\n",
      "\n",
      "In the script above, we pass a sentence in the French language to the\n",
      "TextBlob\n",
      "\n",
      "object. Next, we call the\n",
      "translate\n",
      "\n",
      "method on the object and pass language code\n",
      "en\n",
      "\n",
      "to the\n",
      "to\n",
      "\n",
      "parameter. The language code\n",
      "en\n",
      "\n",
      "corresponds to the English language. In the output, you will see the translation of the French sentence as shown below:\n",
      "\n",
      "Hi, how are you?\n",
      "\n",
      "Let's take another example where we will translate from Arabic to English:\n",
      "\n",
      "text_blob_object_arabic = TextBlob(u'مرحبا كيف حالك؟')\n",
      "print(text_blob_object_arabic.translate(to='en'))\n",
      "\n",
      "Output:\n",
      "\n",
      "Hi, how are you?\n",
      "\n",
      "Finally, using the\n",
      "detect_language\n",
      "\n",
      "method, you can also detect the language of the sentence. Look at the following script:\n",
      "\n",
      "text_blob_object = TextBlob(u'Hola como estas?')\n",
      "print(text_blob_object.detect_language())\n",
      "\n",
      "In the output, you will see\n",
      "es\n",
      "\n",
      ", which stands for the Spanish language.\n",
      "\n",
      "The language code for all the languages can be found at this link.\n",
      "\n",
      "Text Classification\n",
      "\n",
      "TextBlob also provides basic text classification capabilities. Though, I would not recommend TextBlob for text classification owing to its limited capabilities, however, if you have a really limited data and you want to quickly develop a very basic text classification model, then you may use TextBlob. For advanced models, I would recommend machine learning libraries such as Scikit-Learn or Tensorflow.\n",
      "\n",
      "Let's see how we can perform text classification with TextBlob. The first thing we need is a training dataset and test data. The classification model will be trained on the training dataset and will be evaluated on the test dataset.\n",
      "\n",
      "Suppose we have the following training and test data:\n",
      "\n",
      "train_data = [\n",
      "('This is an excellent movie', 'pos'),\n",
      "('The move was fantastic I like it', 'pos'),\n",
      "('You should watch it, it is brilliant', 'pos'),\n",
      "('Exceptionally good', 'pos'),\n",
      "(\"Wonderfully directed and executed. I like it\", 'pos'),\n",
      "('It was very boring', 'neg'),\n",
      "('I did not like the movie', 'neg'),\n",
      "(\"The movie was horrible\", 'neg'),\n",
      "('I will not recommend', 'neg'),\n",
      "('The acting is pathetic', 'neg')\n",
      "]\n",
      "test_data = [\n",
      "('Its a fantastic series', 'pos'),\n",
      "('Never watched such a brillent movie', 'pos'),\n",
      "(\"horrible acting\", 'neg'),\n",
      "(\"It is a Wonderful movie\", 'pos'),\n",
      "('waste of money', 'neg'),\n",
      "(\"pathetic picture\", 'neg')\n",
      "]\n",
      "\n",
      "The dataset contains some dummy reviews about movies. You can see our training and test datasets consist of lists of tuples where the first element of the tuple is the text or a sentence while the second member of the tuple is the corresponding review or sentiment of the text.\n",
      "\n",
      "We will train our dataset on the\n",
      "train_data\n",
      "\n",
      "and will evaluate it on the\n",
      "test_data\n",
      "\n",
      ". To do so, we will use the\n",
      "NaiveBayesClassifier\n",
      "\n",
      "class from the\n",
      "textblob.classifiers\n",
      "\n",
      "library. The following script imports the library:\n",
      "\n",
      "from textblob.classifiers import NaiveBayesClassifier\n",
      "\n",
      "To train the model, we simply have to pass the training data to the constructor of the\n",
      "NaiveBayesClassifier\n",
      "\n",
      "class. The class will return an object trained on the dataset and capable of making predictions on the test set.\n",
      "\n",
      "classifier = NaiveBayesClassifier(train_data)\n",
      "\n",
      "Let's first make a prediction on a single sentence. To do so, we need to call the\n",
      "classify\n",
      "\n",
      "method and pass it the sentence. Look at the following example:\n",
      "\n",
      "print(classifier.classify(\"It is very boring\"))\n",
      "\n",
      "It looks like a negative review. When you execute the above script, you will see\n",
      "neg\n",
      "\n",
      "in the output.\n",
      "\n",
      "Similarly, the following script will return\n",
      "pos\n",
      "\n",
      "since the review is positive.\n",
      "\n",
      "print(classifier.classify(\"It's a fantastic series\"))\n",
      "\n",
      "You can also make a prediction by passing our\n",
      "classifier\n",
      "\n",
      "to the\n",
      "classifier\n",
      "\n",
      "parameter of the\n",
      "TextBlob\n",
      "\n",
      "object. You then have to call the\n",
      "classify\n",
      "\n",
      "method on the\n",
      "TextBlob\n",
      "\n",
      "object to view the prediction.\n",
      "\n",
      "sentence = TextBlob(\"It's a fantastic series.\", classifier=classifier)\n",
      "print(sentence.classify())\n",
      "\n",
      "Finally, to find the accuracy of your algorithm on the test set, call the\n",
      "accuracy\n",
      "\n",
      "method on your classifier and pass it the\n",
      "test_data\n",
      "\n",
      "that we just created. Look at the following script:\n",
      "\n",
      "classifier.accuracy(test_data)\n",
      "\n",
      "In the output, you will see 0.66 which is the accuracy of the algorithm.\n",
      "\n",
      "To find the most important features for the classification, the\n",
      "show_informative_features\n",
      "\n",
      "method can be used. The number of most important features to see is passed as a parameter.\n",
      "\n",
      "classifier.show_informative_features(3)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "Most Informative Features\n",
      "contains(it) = False neg : pos = 2.2 : 1.0\n",
      "contains(is) = True pos : neg = 1.7 : 1.0\n",
      "contains(was) = True neg : pos = 1.7 : 1.0\n",
      "\n",
      "In this section, we tried to find the sentiment of the movie review using text classification. In reality, you don't have to perform text classification to find the sentiment of a sentence in TextBlob. The TextBlob library comes with a built-in sentiment analyzer which we will see in the next section.\n",
      "\n",
      "Sentiment Analysis\n",
      "\n",
      "In this section, we will analyze the sentiment of the public reviews for different foods purchased via Amazon. We will use the TextBlob sentiment analyzer to do so.\n",
      "\n",
      "The dataset can be downloaded from this Kaggle link.\n",
      "\n",
      "As a first step, we need to import the dataset. We will only import the first 20,000 records due to memory constraints. You can import more records if you want. The following script imports the dataset:\n",
      "\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "reviews_datasets = pd.read_csv(r'E:\\Datasets\\Reviews.csv')\n",
      "reviews_datasets = reviews_datasets.head(20000)\n",
      "reviews_datasets.dropna()\n",
      "\n",
      "To see how our dataset looks, we will use the\n",
      "head\n",
      "\n",
      "method of the pandas data frame:\n",
      "\n",
      "reviews_datasets.head()\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "From the output, you can see that the text review about the food is contained by the Text column. The score column contains ratings of the user for the particular product with 1 being the lowest and 5 being the highest rating.\n",
      "\n",
      "Let's see the distribution of rating:\n",
      "\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "%matplotlib inline\n",
      "sns.distplot(reviews_datasets['Score'])\n",
      "\n",
      "You can see that most of the ratings are highly positive i.e. 5. Let's plot the bar plot for the ratings to have a better look at the number of records for each rating.\n",
      "\n",
      "sns.countplot(x='Score', data=reviews_datasets)\n",
      "\n",
      "The output shows that more than half of reviews have 5-star ratings.\n",
      "\n",
      "Let's randomly select a review and find its polarity using TextBlob. Let's take a look at review number 350.\n",
      "\n",
      "reviews_datasets['Text'][350]\n",
      "\n",
      "Output:\n",
      "\n",
      "'These chocolate covered espresso beans are wonderful! The chocolate is very dark and rich and the \"bean\" inside is a very delightful blend of flavors with just enough caffine to really give it a zing.'\n",
      "\n",
      "It looks like the review is positive. Let's verify this using the TextBlob library. To find the sentiment, we have to use the\n",
      "sentiment\n",
      "\n",
      "attribute of the\n",
      "TextBlog\n",
      "\n",
      "object. The\n",
      "sentiment\n",
      "\n",
      "object returns a tuple that contains polarity and subjectivity of the review.\n",
      "\n",
      "The value of polarity can be between -1 and 1 where the reviews with negative polarities have negative sentiments while the reviews with positive polarities have positive sentiments.\n",
      "\n",
      "The subjectivity value can be between 0 and 1. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information.\n",
      "\n",
      "Let's find the sentiment of the 350th review.\n",
      "\n",
      "text_blob_object = TextBlob(reviews_datasets['Text'][350])\n",
      "print(text_blob_object.sentiment)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "Sentiment(polarity=0.39666666666666667,subjectivity=0.6616666666666667)\n",
      "\n",
      "The output shows that the review is positive with a high subjectivity.\n",
      "\n",
      "Let's now add a column for sentiment polarity in our dataset. Execute the following script:\n",
      "\n",
      "def find_pol(review):\n",
      "return TextBlob(review).sentiment.polarity\n",
      "\n",
      "reviews_datasets['Sentiment_Polarity'] = reviews_datasets['Text'].apply(find_pol)\n",
      "reviews_datasets.head()\n",
      "\n",
      "Now let's see the distribution of polarity in our dataset. Execute the following script:\n",
      "\n",
      "sns.distplot(reviews_datasets['Sentiment_Polarity'])\n",
      "\n",
      "The output of the script above looks like this:\n",
      "\n",
      "It is evident from the figure above that most of the reviews are positive and have polarity between 0 and 0.5. This is natural since most of the reviews in the dataset have 5-star ratings.\n",
      "\n",
      "Let's now plot the average polarity for each score rating.\n",
      "\n",
      "sns.barplot(x='Score', y='Sentiment_Polarity', data=reviews_datasets)\n",
      "\n",
      "Output:\n",
      "\n",
      "The output clearly shows that the reviews with high rating scores have high positive polarities.\n",
      "\n",
      "Let's now see some of the most negative reviews i.e. the reviews with a polarity value of -1.\n",
      "\n",
      "most_negative = reviews_datasets[reviews_datasets.Sentiment_Polarity == -1].Text.head()\n",
      "print(most_negative)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "545 These chips are nasty. I thought someone had ...\n",
      "1083 All my fault. I thought this would be a carton...\n",
      "1832 Pop Chips are basically a horribly over-priced...\n",
      "2087 I do not consider Gingerbread, Spicy Eggnog, C...\n",
      "2763 This popcorn has alot of hulls I order 4 bags ...\n",
      "Name: Text, dtype: object\n",
      "\n",
      "Let's print the value of review number 545.\n",
      "\n",
      "reviews_datasets['Text'][545]\n",
      "\n",
      "In the output, you will see the following review:\n",
      "\n",
      "'These chips are nasty. I thought someone had spilled a drink in the bag, no the chips were just soaked with grease. Nasty!!'\n",
      "\n",
      "The output clearly shows that the review is highly negative.\n",
      "\n",
      "Let's now see some of the most positive reviews. Execute the following script:\n",
      "\n",
      "most_positive = reviews_datasets[reviews_datasets.Sentiment_Polarity == 1].Text.head()\n",
      "print(most_positive)\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "106 not what I was expecting in terms of the compa...\n",
      "223 This is an excellent tea. One of the best I h...\n",
      "338 I like a lot of sesame oil and use it in salad...\n",
      "796 My mother and father were the recipient of the...\n",
      "1031 The Kelloggs Muselix are delicious and the del...\n",
      "Name: Text, dtype: object\n",
      "\n",
      "Let's see review 106 in detail:\n",
      "\n",
      "reviews_datasets['Text'][106]\n",
      "\n",
      "Output:\n",
      "\n",
      "\"not what I was expecting in terms of the company's reputation for excellent home delivery products\"\n",
      "\n",
      "You can see that though the review was not very positive, it has been assigned a polarity of 1 due to the presence of words like\n",
      "excellent\n",
      "\n",
      "and\n",
      "reputation\n",
      "\n",
      ". It is important to know that sentiment analyzer is not 100% error-proof and might predict wrong sentiment in a few cases, such as the one we just saw.\n",
      "\n",
      "Let's now see review number 223 which also has been marked as positive.\n",
      "\n",
      "reviews_datasets['Text'][223]\n",
      "\n",
      "The output looks like this:\n",
      "\n",
      "\"This is an excellent tea. One of the best I have ever had. It is especially great when you prepare it with a samovar.\"\n",
      "\n",
      "The output clearly depicts that the review is highly positive.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "Python's TextBlob library is one of the most famous and widely used natural language processing libraries. This article explains several functionalities of the TextBlob library, such as tokenization, stemming, sentiment analysis, text classification and language translation in detail.\n",
      "\n",
      "In the next article I'll go over the Pattern library, which provides a lot of really useful functions for determining attributes about sentences, as well as tools for retrieving data from social networks, Wikipedia, and search engines.\n",
      "\n",
      "python,nlp\n",
      "\n",
      "*\n",
      "*\n",
      "*\n",
      "\n",
      "About Usman Malik\n",
      "\n",
      "Paris (France) Twitter\n",
      "\n",
      "Programmer | Blogger | Data Science Enthusiast | PhD To Be | Arsenal FC for Life\n",
      "\n",
      "Subscribe to our Newsletter\n",
      "\n",
      "Please enable JavaScript to view the comments powered by Disqus.\n",
      "\n",
      "Previous Post\n",
      "\n",
      "Next Post\n",
      "\n",
      "Ad\n",
      "\n",
      "Follow Us\n",
      "\n",
      "Twitter\n",
      "\n",
      "Facebook\n",
      "\n",
      "RSS\n",
      "\n",
      "Data Visualization in Python\n",
      "\n",
      "Understand your data better with visualizations! With over 330+ pages, you'll learn the ins and outs of visualizing data in Python with popular libraries like Matplotlib, Seaborn, Bokeh, and more.\n",
      "\n",
      "Learn more\n",
      "\n",
      "Getting Started with AWS in Node\n",
      "\n",
      "Just released! Build the foundation you'll need to provision, deploy, and run Node.js applications in the AWS cloud. Learn Lambda, EC2, S3, SQS, and more!\n",
      "\n",
      "Learn more\n",
      "\n",
      "Git Essentials\n",
      "\n",
      "Just released! Check out this hands-on, practical guide to learning Git, with best-practices and industry-accepted standards. Stop Googling Git commands and actually learn it!\n",
      "\n",
      "Learn more\n",
      "\n",
      "Newsletter\n",
      "\n",
      "Subscribe to our newsletter! Get occassional tutorials, guides, and reviews in your inbox.\n",
      "\n",
      "Ad\n",
      "\n",
      "Want a remote job?\n",
      "\n",
      "More jobs\n",
      "\n",
      "Jobs via\n",
      "HireRemote.io\n",
      "\n",
      "Prepping for an interview?\n",
      "\n",
      "* Improve your skills by solving one coding problem every day\n",
      "\n",
      "* Get the solutions the next morning via email\n",
      "\n",
      "* Practice on actual problems asked by top companies, like:\n",
      "\n",
      "Daily Coding Problem\n",
      "\n",
      "Ad\n",
      "\n",
      "Recent Posts\n",
      "\n",
      "Covariance and Correlation in Python\n",
      "\n",
      "Python: Get Size of Dictionary\n",
      "\n",
      "Form Data Validation in Node.js with express-validator\n",
      "\n",
      "Tags\n",
      "\n",
      "aialgorithmsamqpangularannouncementsapacheapache commonsapiarduinoartificial intelligence\n",
      "\n",
      "Follow Us\n",
      "\n",
      "Twitter\n",
      "\n",
      "Facebook\n",
      "\n",
      "RSS\n",
      "\n",
      "Copyright © 2021, Stack Abuse. All Rights Reserved.\n",
      "\n",
      "Disclosure•Privacy Policy•Terms of Service\n"
     ]
    }
   ],
   "source": [
    "from pattern.web import URL, plaintext\n",
    "\n",
    "html_content = URL('https://stackabuse.com/python-for-nlp-introduction-to-the-textblob-library/').download()\n",
    "cleaned_page = plaintext(html_content.decode('utf-8'))\n",
    "print(cleaned_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing PDF Documments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pattern PDF module (doesn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This doesn't work\n",
    "# from pattern.web import URL, PDF\n",
    "\n",
    "# pdf_doc = URL('http://demo.clab.cs.cmu.edu/NLP/syllabus_f18.pdf').download()\n",
    "# # pdf_doc2 = URL('https://courses.cs.ut.ee/LTAT.01.001/2020_spring/uploads/Main/Lecture1_Introduction.pdf').download()\n",
    "# print(PDF(pdf_doc2.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using PyPDF2 library [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dependencies\n",
    "# !pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedReader name='data/syllabus_f18.pdf'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import PyPDF2 as pdf\n",
    "\n",
    "file = open('data/syllabus_f18.pdf', 'rb') # source: http://demo.clab.cs.cmu.edu/NLP/syllabus_f18.pdf\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyPDF2.pdf.PdfFileReader at 0x12fd6a6a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader = pdf.PdfFileReader(file)\n",
    "pdf_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PdfFileReader in module PyPDF2.pdf object:\n",
      "\n",
      "class PdfFileReader(builtins.object)\n",
      " |  Initializes a PdfFileReader object.  This operation can take some time, as\n",
      " |  the PDF stream's cross-reference tables are read into memory.\n",
      " |  \n",
      " |  :param stream: A File object or an object that supports the standard read\n",
      " |      and seek methods similar to a File object. Could also be a\n",
      " |      string representing a path to a PDF file.\n",
      " |  :param bool strict: Determines whether user should be warned of all\n",
      " |      problems and also causes some correctable problems to be fatal.\n",
      " |      Defaults to ``True``.\n",
      " |  :param warndest: Destination for logging warnings (defaults to\n",
      " |      ``sys.stderr``).\n",
      " |  :param bool overwriteWarnings: Determines whether to override Python's\n",
      " |      ``warnings.py`` module with a custom implementation (defaults to\n",
      " |      ``True``).\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, stream, strict=True, warndest=None, overwriteWarnings=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  cacheGetIndirectObject(self, generation, idnum)\n",
      " |  \n",
      " |  cacheIndirectObject(self, generation, idnum, obj)\n",
      " |  \n",
      " |  decrypt(self, password)\n",
      " |      When using an encrypted / secured PDF file with the PDF Standard\n",
      " |      encryption handler, this function will allow the file to be decrypted.\n",
      " |      It checks the given password against the document's user password and\n",
      " |      owner password, and then stores the resulting decryption key if either\n",
      " |      password is correct.\n",
      " |      \n",
      " |      It does not matter which password was matched.  Both passwords provide\n",
      " |      the correct decryption key that will allow the document to be used with\n",
      " |      this library.\n",
      " |      \n",
      " |      :param str password: The password to match.\n",
      " |      :return: ``0`` if the password failed, ``1`` if the password matched the user\n",
      " |          password, and ``2`` if the password matched the owner password.\n",
      " |      :rtype: int\n",
      " |      :raises NotImplementedError: if document uses an unsupported encryption\n",
      " |          method.\n",
      " |  \n",
      " |  getDestinationPageNumber(self, destination)\n",
      " |      Retrieve page number of a given Destination object\n",
      " |      \n",
      " |      :param Destination destination: The destination to get page number.\n",
      " |           Should be an instance of\n",
      " |           :class:`Destination<PyPDF2.pdf.Destination>`\n",
      " |      :return: the page number or -1 if page not found\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  getDocumentInfo(self)\n",
      " |      Retrieves the PDF file's document information dictionary, if it exists.\n",
      " |      Note that some PDF files use metadata streams instead of docinfo\n",
      " |      dictionaries, and these metadata streams will not be accessed by this\n",
      " |      function.\n",
      " |      \n",
      " |      :return: the document information of this PDF file\n",
      " |      :rtype: :class:`DocumentInformation<pdf.DocumentInformation>` or ``None`` if none exists.\n",
      " |  \n",
      " |  getFields(self, tree=None, retval=None, fileobj=None)\n",
      " |      Extracts field data if this PDF contains interactive form fields.\n",
      " |      The *tree* and *retval* parameters are for recursive use.\n",
      " |      \n",
      " |      :param fileobj: A file object (usually a text file) to write\n",
      " |          a report to on all interactive form fields found.\n",
      " |      :return: A dictionary where each key is a field name, and each\n",
      " |          value is a :class:`Field<PyPDF2.generic.Field>` object. By\n",
      " |          default, the mapping name is used for keys.\n",
      " |      :rtype: dict, or ``None`` if form data could not be located.\n",
      " |  \n",
      " |  getFormTextFields(self)\n",
      " |      Retrieves form fields from the document with textual data (inputs, dropdowns)\n",
      " |  \n",
      " |  getIsEncrypted(self)\n",
      " |  \n",
      " |  getNamedDestinations(self, tree=None, retval=None)\n",
      " |      Retrieves the named destinations present in the document.\n",
      " |      \n",
      " |      :return: a dictionary which maps names to\n",
      " |          :class:`Destinations<PyPDF2.generic.Destination>`.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  getNumPages(self)\n",
      " |      Calculates the number of pages in this PDF file.\n",
      " |      \n",
      " |      :return: number of pages\n",
      " |      :rtype: int\n",
      " |      :raises PdfReadError: if file is encrypted and restrictions prevent\n",
      " |          this action.\n",
      " |  \n",
      " |  getObject(self, indirectReference)\n",
      " |  \n",
      " |  getOutlines(self, node=None, outlines=None)\n",
      " |      Retrieves the document outline present in the document.\n",
      " |      \n",
      " |      :return: a nested list of :class:`Destinations<PyPDF2.generic.Destination>`.\n",
      " |  \n",
      " |  getPage(self, pageNumber)\n",
      " |      Retrieves a page by number from this PDF file.\n",
      " |      \n",
      " |      :param int pageNumber: The page number to retrieve\n",
      " |          (pages begin at zero)\n",
      " |      :return: a :class:`PageObject<pdf.PageObject>` instance.\n",
      " |      :rtype: :class:`PageObject<pdf.PageObject>`\n",
      " |  \n",
      " |  getPageLayout(self)\n",
      " |      Get the page layout.\n",
      " |      See :meth:`setPageLayout()<PdfFileWriter.setPageLayout>`\n",
      " |      for a description of valid layouts.\n",
      " |      \n",
      " |      :return: Page layout currently being used.\n",
      " |      :rtype: ``str``, ``None`` if not specified\n",
      " |  \n",
      " |  getPageMode(self)\n",
      " |      Get the page mode.\n",
      " |      See :meth:`setPageMode()<PdfFileWriter.setPageMode>`\n",
      " |      for a description of valid modes.\n",
      " |      \n",
      " |      :return: Page mode currently being used.\n",
      " |      :rtype: ``str``, ``None`` if not specified\n",
      " |  \n",
      " |  getPageNumber(self, page)\n",
      " |      Retrieve page number of a given PageObject\n",
      " |      \n",
      " |      :param PageObject page: The page to get page number. Should be\n",
      " |          an instance of :class:`PageObject<PyPDF2.pdf.PageObject>`\n",
      " |      :return: the page number or -1 if page not found\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  getXmpMetadata(self)\n",
      " |      Retrieves XMP (Extensible Metadata Platform) data from the PDF document\n",
      " |      root.\n",
      " |      \n",
      " |      :return: a :class:`XmpInformation<xmp.XmpInformation>`\n",
      " |          instance that can be used to access XMP metadata from the document.\n",
      " |      :rtype: :class:`XmpInformation<xmp.XmpInformation>` or\n",
      " |          ``None`` if no metadata was found on the document root.\n",
      " |  \n",
      " |  read(self, stream)\n",
      " |  \n",
      " |  readNextEndLine(self, stream)\n",
      " |  \n",
      " |  readObjectHeader(self, stream)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  documentInfo\n",
      " |  \n",
      " |  isEncrypted\n",
      " |  \n",
      " |  namedDestinations\n",
      " |  \n",
      " |  numPages\n",
      " |  \n",
      " |  outlines\n",
      " |  \n",
      " |  pageLayout\n",
      " |      Get the page layout.\n",
      " |      See :meth:`setPageLayout()<PdfFileWriter.setPageLayout>`\n",
      " |      for a description of valid layouts.\n",
      " |      \n",
      " |      :return: Page layout currently being used.\n",
      " |      :rtype: ``str``, ``None`` if not specified\n",
      " |  \n",
      " |  pageMode\n",
      " |      Get the page mode.\n",
      " |      See :meth:`setPageMode()<PdfFileWriter.setPageMode>`\n",
      " |      for a description of valid modes.\n",
      " |      \n",
      " |      :return: Page mode currently being used.\n",
      " |      :rtype: ``str``, ``None`` if not specified\n",
      " |  \n",
      " |  pages\n",
      " |  \n",
      " |  xmpMetadata\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pdf_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader.getIsEncrypted()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PDF is not encrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader.getNumPages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Type': '/Page',\n",
       " '/Contents': {'/Filter': '/FlateDecode'},\n",
       " '/Resources': {'/Font': {'/F16': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/IFHENN+CMR17',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/IFHENN+CMR17',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-33, -250, 945, 749],\n",
       "     '/Ascent': 694,\n",
       "     '/CapHeight': 683,\n",
       "     '/Descent': -195,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 53,\n",
       "     '/XHeight': 430,\n",
       "     '/CharSet': '/L/N/P/S/a/b/c/colon/e/g/i/l/n/o/r/s/t/u/y',\n",
       "     '/FontFile': {'/Length1': 1629,\n",
       "      '/Length2': 9058,\n",
       "      '/Length3': 0,\n",
       "      '/Filter': '/FlateDecode'}},\n",
       "    '/FirstChar': 58,\n",
       "    '/LastChar': 121,\n",
       "    '/Widths': [249.6,\n",
       "     249.6,\n",
       "     249.6,\n",
       "     719.8,\n",
       "     432.5,\n",
       "     432.5,\n",
       "     719.8,\n",
       "     693.3,\n",
       "     654.3,\n",
       "     667.6,\n",
       "     706.6,\n",
       "     628.2,\n",
       "     602.1,\n",
       "     726.3,\n",
       "     693.3,\n",
       "     327.6,\n",
       "     471.5,\n",
       "     719.4,\n",
       "     576,\n",
       "     850,\n",
       "     693.3,\n",
       "     719.8,\n",
       "     628.2,\n",
       "     719.8,\n",
       "     680.5,\n",
       "     510.9,\n",
       "     667.6,\n",
       "     693.3,\n",
       "     693.3,\n",
       "     954.5,\n",
       "     693.3,\n",
       "     693.3,\n",
       "     563.1,\n",
       "     249.6,\n",
       "     458.6,\n",
       "     249.6,\n",
       "     458.6,\n",
       "     249.6,\n",
       "     249.6,\n",
       "     458.6,\n",
       "     510.9,\n",
       "     406.4,\n",
       "     510.9,\n",
       "     406.4,\n",
       "     275.8,\n",
       "     458.6,\n",
       "     510.9,\n",
       "     249.6,\n",
       "     275.8,\n",
       "     484.7,\n",
       "     249.6,\n",
       "     772.1,\n",
       "     510.9,\n",
       "     458.6,\n",
       "     510.9,\n",
       "     484.7,\n",
       "     354.1,\n",
       "     359.4,\n",
       "     354.1,\n",
       "     510.9,\n",
       "     484.7,\n",
       "     667.6,\n",
       "     484.7,\n",
       "     484.7]},\n",
       "   '/F17': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/JBWCNI+CMR12',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/JBWCNI+CMR12',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-34, -251, 988, 750],\n",
       "     '/Ascent': 694,\n",
       "     '/CapHeight': 683,\n",
       "     '/Descent': -194,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 65,\n",
       "     '/XHeight': 431,\n",
       "     '/CharSet': '/A/B/C/D/F/M/R/U/W/a/ampersand/c/d/e/eight/g/i/k/l/n/o/one/period/r/s/t/two/v/y/zero',\n",
       "     '/FontFile': {'/Length1': 1805,\n",
       "      '/Length2': 11451,\n",
       "      '/Length3': 0,\n",
       "      '/Filter': '/FlateDecode'}},\n",
       "    '/FirstChar': 38,\n",
       "    '/LastChar': 121,\n",
       "    '/Widths': [761.6,\n",
       "     272,\n",
       "     380.8,\n",
       "     380.8,\n",
       "     489.6,\n",
       "     761.6,\n",
       "     272,\n",
       "     326.4,\n",
       "     272,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     489.6,\n",
       "     272,\n",
       "     272,\n",
       "     272,\n",
       "     761.6,\n",
       "     462.4,\n",
       "     462.4,\n",
       "     761.6,\n",
       "     734,\n",
       "     693.4,\n",
       "     707.2,\n",
       "     747.8,\n",
       "     666.2,\n",
       "     639,\n",
       "     768.3,\n",
       "     734,\n",
       "     353.2,\n",
       "     503,\n",
       "     761.2,\n",
       "     611.8,\n",
       "     897.2,\n",
       "     734,\n",
       "     761.6,\n",
       "     666.2,\n",
       "     761.6,\n",
       "     720.6,\n",
       "     544,\n",
       "     707.2,\n",
       "     734,\n",
       "     734,\n",
       "     1006,\n",
       "     734,\n",
       "     734,\n",
       "     598.4,\n",
       "     272,\n",
       "     489.6,\n",
       "     272,\n",
       "     489.6,\n",
       "     272,\n",
       "     272,\n",
       "     489.6,\n",
       "     544,\n",
       "     435.2,\n",
       "     544,\n",
       "     435.2,\n",
       "     299.2,\n",
       "     489.6,\n",
       "     544,\n",
       "     272,\n",
       "     299.2,\n",
       "     516.8,\n",
       "     272,\n",
       "     816,\n",
       "     544,\n",
       "     489.6,\n",
       "     544,\n",
       "     516.8,\n",
       "     380.8,\n",
       "     386.2,\n",
       "     380.8,\n",
       "     544,\n",
       "     516.8,\n",
       "     707.2,\n",
       "     516.8,\n",
       "     516.8]},\n",
       "   '/F40': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/WPQPJO+CMTI10',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/WPQPJO+CMTI10',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-35, -250, 1124, 750],\n",
       "     '/Ascent': 694,\n",
       "     '/CapHeight': 683,\n",
       "     '/Descent': -194,\n",
       "     '/ItalicAngle': -14,\n",
       "     '/StemV': 68,\n",
       "     '/XHeight': 431,\n",
       "     '/CharSet': '/A/C/F/I/L/N/O/P/R/S/T/W/a/b/c/colon/comma/d/e/ffi/g/h/i/l/m/n/o/p/r/s/t/u/y',\n",
       "     '/FontFile': {'/Length1': 1849,\n",
       "      '/Length2': 13937,\n",
       "      '/Length3': 0,\n",
       "      '/Filter': '/FlateDecode'}},\n",
       "    '/FirstChar': 14,\n",
       "    '/LastChar': 121,\n",
       "    '/Widths': [881.7,\n",
       "     894.4,\n",
       "     306.7,\n",
       "     332.2,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     831.3,\n",
       "     460,\n",
       "     536.7,\n",
       "     715.6,\n",
       "     715.6,\n",
       "     511.1,\n",
       "     882.8,\n",
       "     985,\n",
       "     766.7,\n",
       "     255.6,\n",
       "     306.7,\n",
       "     514.4,\n",
       "     817.8,\n",
       "     769.1,\n",
       "     817.8,\n",
       "     766.7,\n",
       "     306.7,\n",
       "     408.9,\n",
       "     408.9,\n",
       "     511.1,\n",
       "     766.7,\n",
       "     306.7,\n",
       "     357.8,\n",
       "     306.7,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     306.7,\n",
       "     306.7,\n",
       "     306.7,\n",
       "     766.7,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     766.7,\n",
       "     743.3,\n",
       "     703.9,\n",
       "     715.6,\n",
       "     755,\n",
       "     678.3,\n",
       "     652.8,\n",
       "     773.6,\n",
       "     743.3,\n",
       "     385.6,\n",
       "     525,\n",
       "     768.9,\n",
       "     627.2,\n",
       "     896.7,\n",
       "     743.3,\n",
       "     766.7,\n",
       "     678.3,\n",
       "     766.7,\n",
       "     729.4,\n",
       "     562.2,\n",
       "     715.6,\n",
       "     743.3,\n",
       "     743.3,\n",
       "     998.9,\n",
       "     743.3,\n",
       "     743.3,\n",
       "     613.3,\n",
       "     306.7,\n",
       "     514.4,\n",
       "     306.7,\n",
       "     511.1,\n",
       "     306.7,\n",
       "     306.7,\n",
       "     511.1,\n",
       "     460,\n",
       "     460,\n",
       "     511.1,\n",
       "     460,\n",
       "     306.7,\n",
       "     460,\n",
       "     511.1,\n",
       "     306.7,\n",
       "     306.7,\n",
       "     460,\n",
       "     255.6,\n",
       "     817.8,\n",
       "     562.2,\n",
       "     511.1,\n",
       "     511.1,\n",
       "     460,\n",
       "     421.7,\n",
       "     408.9,\n",
       "     332.2,\n",
       "     536.7,\n",
       "     460,\n",
       "     664.4,\n",
       "     463.9,\n",
       "     485.6]},\n",
       "   '/F15': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/DMFBIO+CMR10',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/DMFBIO+CMR10',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-40, -250, 1009, 750],\n",
       "     '/Ascent': 694,\n",
       "     '/CapHeight': 683,\n",
       "     '/Descent': -194,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 69,\n",
       "     '/XHeight': 431,\n",
       "     '/CharSet': '/A/B/C/D/E/F/H/I/J/L/M/N/P/Q/R/S/T/U/V/W/Y/a/ampersand/b/bracketleft/bracketright/c/colon/comma/d/e/eight/endash/f/ff/ffi/fi/five/fl/four/g/h/hyphen/i/j/k/l/m/n/nine/o/one/p/parenleft/parenright/percent/period/q/question/quotedblleft/quotedblright/quoteright/r/s/semicolon/seven/six/t/three/two/u/v/w/x/y/z/zero',\n",
       "     '/FontFile': {'/Length1': 2608,\n",
       "      '/Length2': 22640,\n",
       "      '/Length3': 0,\n",
       "      '/Filter': '/FlateDecode'}},\n",
       "    '/FirstChar': 11,\n",
       "    '/LastChar': 123,\n",
       "    '/Widths': [583.3,\n",
       "     555.6,\n",
       "     555.6,\n",
       "     833.3,\n",
       "     833.3,\n",
       "     277.8,\n",
       "     305.6,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     750,\n",
       "     444.4,\n",
       "     500,\n",
       "     722.2,\n",
       "     777.8,\n",
       "     500,\n",
       "     902.8,\n",
       "     1013.9,\n",
       "     777.8,\n",
       "     277.8,\n",
       "     277.8,\n",
       "     500,\n",
       "     833.3,\n",
       "     500,\n",
       "     833.3,\n",
       "     777.8,\n",
       "     277.8,\n",
       "     388.9,\n",
       "     388.9,\n",
       "     500,\n",
       "     777.8,\n",
       "     277.8,\n",
       "     333.3,\n",
       "     277.8,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     277.8,\n",
       "     277.8,\n",
       "     277.8,\n",
       "     777.8,\n",
       "     472.2,\n",
       "     472.2,\n",
       "     777.8,\n",
       "     750,\n",
       "     708.3,\n",
       "     722.2,\n",
       "     763.9,\n",
       "     680.6,\n",
       "     652.8,\n",
       "     784.7,\n",
       "     750,\n",
       "     361.1,\n",
       "     513.9,\n",
       "     777.8,\n",
       "     625,\n",
       "     916.7,\n",
       "     750,\n",
       "     777.8,\n",
       "     680.6,\n",
       "     777.8,\n",
       "     736.1,\n",
       "     555.6,\n",
       "     722.2,\n",
       "     750,\n",
       "     750,\n",
       "     1027.8,\n",
       "     750,\n",
       "     750,\n",
       "     611.1,\n",
       "     277.8,\n",
       "     500,\n",
       "     277.8,\n",
       "     500,\n",
       "     277.8,\n",
       "     277.8,\n",
       "     500,\n",
       "     555.6,\n",
       "     444.4,\n",
       "     555.6,\n",
       "     444.4,\n",
       "     305.6,\n",
       "     500,\n",
       "     555.6,\n",
       "     277.8,\n",
       "     305.6,\n",
       "     527.8,\n",
       "     277.8,\n",
       "     833.3,\n",
       "     555.6,\n",
       "     500,\n",
       "     555.6,\n",
       "     527.8,\n",
       "     391.7,\n",
       "     394.4,\n",
       "     388.9,\n",
       "     555.6,\n",
       "     527.8,\n",
       "     722.2,\n",
       "     527.8,\n",
       "     527.8,\n",
       "     444.4,\n",
       "     500]},\n",
       "   '/F41': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/VMVUAL+CMTT10',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/VMVUAL+CMTT10',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-4, -233, 537, 696],\n",
       "     '/Ascent': 611,\n",
       "     '/CapHeight': 611,\n",
       "     '/Descent': -222,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 69,\n",
       "     '/XHeight': 431,\n",
       "     '/CharSet': '/C/L/N/P/a/at/b/c/colon/d/e/f/g/h/i/k/l/m/n/o/p/period/r/s/slash/t/u/v/w/y',\n",
       "     '/FontFile': {'/Length1': 1807,\n",
       "      '/Length2': 11752,\n",
       "      '/Length3': 0,\n",
       "      '/Filter': '/FlateDecode'}},\n",
       "    '/FirstChar': 46,\n",
       "    '/LastChar': 121,\n",
       "    '/Widths': [525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525,\n",
       "     525]},\n",
       "   '/F43': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type1',\n",
       "    '/BaseFont': '/WYBMPK+CMBX12',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/WYBMPK+CMBX12',\n",
       "     '/Flags': 4,\n",
       "     '/FontBBox': [-53, -251, 1139, 750],\n",
       "     '/Ascent': 694,\n",
       "     '/CapHeight': 686,\n",
       "     '/Descent': -194,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/StemV': 109,\n",
       "     '/XHeight': 444,\n",
       "     '/CharSet': '/E/L/P/S/T/a/b/c/e/five/four/g/i/j/k/l/m/n/o/one/r/s/six/t/three/two/u/v/x/y',\n",
       "     '/FontFile': {'/Length1': 1807,\n",
       "      '/Length2': 11073,\n",
       "      '/Length3': 0,\n",
       "      '/Filter': '/FlateDecode'}},\n",
       "    '/FirstChar': 49,\n",
       "    '/LastChar': 121,\n",
       "    '/Widths': [562.5,\n",
       "     562.5,\n",
       "     562.5,\n",
       "     562.5,\n",
       "     562.5,\n",
       "     562.5,\n",
       "     562.5,\n",
       "     562.5,\n",
       "     562.5,\n",
       "     312.5,\n",
       "     312.5,\n",
       "     342.6,\n",
       "     875,\n",
       "     531.2,\n",
       "     531.2,\n",
       "     875,\n",
       "     849.5,\n",
       "     799.8,\n",
       "     812.5,\n",
       "     862.3,\n",
       "     738.4,\n",
       "     707.2,\n",
       "     884.3,\n",
       "     879.6,\n",
       "     419,\n",
       "     581,\n",
       "     880.8,\n",
       "     675.9,\n",
       "     1067.1,\n",
       "     879.6,\n",
       "     844.9,\n",
       "     768.5,\n",
       "     844.9,\n",
       "     839.1,\n",
       "     625,\n",
       "     782.4,\n",
       "     864.6,\n",
       "     849.5,\n",
       "     1162,\n",
       "     849.5,\n",
       "     849.5,\n",
       "     687.5,\n",
       "     312.5,\n",
       "     581,\n",
       "     312.5,\n",
       "     562.5,\n",
       "     312.5,\n",
       "     312.5,\n",
       "     546.9,\n",
       "     625,\n",
       "     500,\n",
       "     625,\n",
       "     513.3,\n",
       "     343.7,\n",
       "     562.5,\n",
       "     625,\n",
       "     312.5,\n",
       "     343.7,\n",
       "     593.7,\n",
       "     312.5,\n",
       "     937.5,\n",
       "     625,\n",
       "     562.5,\n",
       "     625,\n",
       "     593.7,\n",
       "     459.5,\n",
       "     443.8,\n",
       "     437.5,\n",
       "     625,\n",
       "     593.7,\n",
       "     812.5,\n",
       "     593.7,\n",
       "     593.7]}},\n",
       "  '/ProcSet': ['/PDF', '/Text']},\n",
       " '/MediaBox': [0, 0, 612, 792],\n",
       " '/Parent': {'/Type': '/Pages',\n",
       "  '/Count': 4,\n",
       "  '/Kids': [IndirectObject(27, 0),\n",
       "   IndirectObject(52, 0),\n",
       "   IndirectObject(65, 0),\n",
       "   IndirectObject(73, 0)]},\n",
       " '/Annots': [IndirectObject(28, 0),\n",
       "  IndirectObject(29, 0),\n",
       "  IndirectObject(45, 0),\n",
       "  IndirectObject(30, 0),\n",
       "  IndirectObject(31, 0),\n",
       "  IndirectObject(32, 0),\n",
       "  IndirectObject(46, 0),\n",
       "  IndirectObject(33, 0),\n",
       "  IndirectObject(34, 0),\n",
       "  IndirectObject(35, 0)]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page1 = pdf_reader.getPage(0)\n",
    "page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NaturalLanguageProcessing:Syllabus\\nAlanW.Black&DavidR.Mortensen\\nCarnegieMellonUniversity\\nFall2018\\nInstructors:\\nProf.AlanWBlack(\\nawb@cs.cmu.edu\\n)andDavidR.Mortensen(\\ndmortens@\\ncs.cmu.edu\\n)\\nTeachingassistants:\\nFatimaAl-Raisi(\\nfraisi@andrew.cmu.edu\\n),ManishaChaurasia\\n(\\nmchauras@andrew.cmu.edu\\n),PoojaChitkara(\\npchitkar@andrew.cmu.\\nedu\\n),SarveshwaranDhansekar(\\nsarveshd@andrew.cmu.edu\\n)\\nLecturetime:\\nTuesdays&Thursdays,3:00{4:20\\nLocation:\\nWEH4623\\nWebpage:\\nhttp://demo.clab.cs.cmu.edu/NLP/\\nFacultyehours:\\nByappointment(Black);\\nByappointmentat\\nhttps://davidmortensen.youcanbook.me\\n(Mortensen)\\nTAehours:\\nTBA\\n1Summary\\nThiscourseisaboutavarietyofwaystorepresenthumanlanguages(likeEnglishandChinese)as\\ncomputationalsystems,andhowtoexploitthoserepresentationstowriteprogramsthatdouseful\\nthingswithtextandspeechdata,liketranslation,summarization,extractinginformation,question\\nanswering,naturalinterfacestodatabases,andconversationalagents.\\nThisiscalledNaturalLanguageProcessingorComputationalLinguistics,anditisex-\\ntremelymultidisciplinary.ThiscoursewillthereforeincludesomeideascentraltoMachineLearning\\n(discreteclassiprobabilitymodels)andtoLinguistics(morphology,syntax,semantics).\\nWe'llcovercomputationaltreatmentsofwords,sounds,sentences,meanings,andconversations.\\nWe'llseehowprobabilitiesandreal-worldtextdatacanhelp.We'llseehowtlevelsinteract\\ninstate-of-the-artapproachestoapplicationsliketranslationandinformationextraction.\\nFromasoftwareengineeringperspective,therewillbeanemphasisonrapidprototyping,auseful\\nskillinmanyotherareasofComputerScience.Inparticular,wewillintroducesomehigh-level\\nformalisms(e.g.,regularexpressions)andtools(e.g.,Python)thatcangreatlysimplifyprototype\\nimplementation.\\n2Target\\nThecourseisdesignedforSCSundergraduatestudents,andalsotostudentsingraduateprograms\\nwhohaveaperipheralinterestinnaturallanguage,orlinguisticsstudentswhoknowhowtopro-\\n1\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page1.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gram.Prerequisite:FundamentalDataStructuresandAlgorithms(15-211)orequivalent;strong\\nprogrammingcapabilities.\\n3Evaluation\\nStudentswillbeevaluatedineways:\\nExams(40%)\\nonein-classmidtermon\\nMarch\\n(20%)andonecumulativeexam(20%),date\\nTBD.\\nProject(30%)\\nasemester-long4-personteamproject(seebelow).\\nHomeworkassignments(20%)\\n7pencil-and-paperorsmallprogrammingproblemsgivenroughly\\nweekly.\\nQuizzes(10%)\\n10Canvasquizzesgivenatthebeginningofmanylectures\\n1\\n.\\nThelowest2homeworkgradesandthelowest3quizgradeswillbedropped.\\nLatePolicy\\nNoworkwillbeacceptedlate.Thegradingpolicyforpopquizzesandhomework\\nassignmentspermitssomeslackofanadministrativelysimplerkindthandeductingpointsfor\\nlatenessormissingalecture.\\nAcademicHonesty\\nExamsandpopquizzesaretobecompletedindividually.Verbalcollab-\\norationonhomeworkassignmentsisacceptable,but(a)youmustnotshareanycodeorother\\nwrittenmaterial,(b)everythingyouturninmustbeyourownwork,and(c)youmustnotethe\\nnamesof\\nanyone\\nyoucollaboratedwithoneachproblem(the\\nonly\\nexceptionsaretheinstructor\\nandTA),andthenatureofthecollaboration(e.g.,\\\\\\nX\\nhelpedme,\"\\\\Ihelped\\nX\\n,\"\\\\\\nX\\nandIworked\\nitouttogether.\").Ifyoumaterialinpublishedliterature(e.g.,ontheWeb)thatishelpfulin\\nsolvingaproblem,youmustciteitandexplaintheanswerinyourownwords.Theprojectisto\\nbecompletedbyateam;youarenotpermittedtodiscussanyaspectofyourprojectwithanyone\\notherthanyourteammembers,theinstructor,andtheTA.Youareencouragedtouseexisting\\nNLPcomponentsinyourproject;youmustacknowledgetheseappropriatelyinthedocumentation.\\nSuspectedviolationsoftheseruleswillbehandledinaccordancewiththeCMUguidelineson\\ncollaborationandcheating(\\nhttp://www.cmu.edu/policies/documents/Cheating.html\\n).\\n4Project\\nAmajorcomponentwillbea4-personteamproject.Theprojectinvolvestwoparts:\\n\\na\\nquestioning\\nprogram(\\nask\\n)whoseinputisawebpage\\nP\\nandwhoseoutputisasetof\\nquestionsaboutthecontentin\\nP\\nthatahumancouldanswerifsheread\\nP\\n,and\\n\\na\\nanswering\\nprogram(\\nanswer\\n)whoseinputisawebpage\\nP\\nandaquestion\\nQ\\nabout\\nP\\nand\\nwhoseoutputisanintelligentanswer\\nA\\n.\\n1\\nStudentsshouldbringadevicetoclasssotheycanaccesCanvas.\\n2\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page2 = pdf_reader.getPage(1)\n",
    "page2.extractText()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing the Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.web import cache\n",
    "\n",
    "cache.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
